["","\nThe Deep Learning textbook is a resource intended to help students\nand practitioners enter the field of machine learning in general\nand deep learning in particular.\nThe online version of the book is now complete and will remain\navailable online for free.\n","In five courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach. You will see and work on case studies in healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will also build near state-of-the-art deep learning models for several of these applications. In a \"Machine Learning flight simulator\", you will work through case studies and gain \"industry-like experience\" setting direction for an ML team.  is also partnering with the NVIDIA Deep Learning Institute (DLI) to provide labs in advanced, application-specific topics and to give learners access to GPUs for programming assignments. This will give you an opportunity to build deep learning projects in a cutting-edge, industry-like environment.This course will teach you how to build models for natural language, audio, and other sequence data. Thanks to deep learning, sequence algorithms are working far better than just two years ago, and this is enabling numerous exciting applications in speech recognition, music synthesis, chatbots, machine translation, natural language understanding, and many others. \n\nYou will:\n- Understand how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs.\n- Be able to apply sequence models to natural language problems, including text synthesis. \n- Be able to apply sequence models to audio applications, including speech recognition and music synthesis.\n\nThis is the fifth and final course of the Deep Learning Specialization.","Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing \"Go\").Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.","We'll start our account of convolutional networks with the shallow\nnetworks used to attack this problem earlier in the book.  Through\nmany iterations we'll build up more and more powerful networks.  As we\ngo we'll explore many powerful techniques: convolutions, pooling, the\nuse of GPUs to do far more training than we did with our shallow\nnetworks, the algorithmic expansion of our training data (to reduce\noverfitting), the use of the dropout technique (also to reduce\noverfitting), the use of ensembles of networks, and others.  The\nresult will be a system that offers near-human performance.  Of the\n10,000 MNIST test images - images not seen during training! - our\nsystem will classify 9,967 correctly.  Here's a peek at the 33 images\nwhich are misclassified.  Note that the correct classification is in\nthe top right; our program's classification is in the bottom right:It's worth noting what the chapter is not.  It's not a tutorial on the\nlatest and greatest neural networks libraries.  Nor are we going to be\ntraining deep networks with dozens of layers to solve problems at the\nvery leading edge.  Rather, the focus is on understanding some of the\ncore principles behind deep neural networks, and applying them in the\nsimple, easy-to-understand context of the MNIST problem.  Put another\nway: the chapter is not going to bring you right up to the frontier.\nRather, the intent of this and earlier chapters is to focus on\nfundamentals, and so to prepare you to understand a wide range of\ncurrent work.We then slide the local receptive field across the entire input image.\nFor each local receptive field, there is a different hidden neuron in\nthe first hidden layer.  To illustrate this concretely, let's start\nwith a local receptive field in the top-left corner:\n\n*I haven't precisely defined the\n  notion of a feature.  Informally, think of the feature detected by a\n  hidden neuron as the kind of input pattern that will cause the\n  neuron to activate: it might be an edge in the image, for instance,\n  or maybe some other type of shape. \n*In\n  fact, for the MNIST digit classification problem we've been\n  studying, the images are centered and size-normalized.  So MNIST has\n  less translation invariance than images found \"in the wild\", so to\n  speak.  Still, features like edges and corners are likely to be\n  useful across much of the input space. The network structure I've described so far can detect just a single\nkind of localized feature.  To do image recognition we'll need more\nthan one feature map.  And so a complete convolutional layer consists\nof several different feature maps:Of course, we can't really do a direct comparison between the number\nof parameters, since the two models are different in essential ways.\nBut, intuitively, it seems likely that the use of translation\ninvariance by the convolutional layer will reduce the number of\nparameters it needs to get the same performance as the fully-connected\nmodel.  That, in turn, will result in faster training for the\nconvolutional model, and, ultimately, will help us build deep networks\nusing convolutional layers.\n*The\n  nomenclature is being used loosely here.  In particular, I'm using\n  \"feature map\" to mean not the function computed by the\n  convolutional layer, but rather the activation of the hidden neurons\n  output from the layer.  This kind of mild abuse of nomenclature is\n  pretty common in the research literature.As mentioned above, the convolutional layer usually involves more than\na  single feature  map.   We  apply max-pooling  to  each feature  map\nseparately.   So  if  there  were three  feature  maps,  the  combined\nconvolutional and max-pooling layers would look like:We can think of max-pooling as a way for the network to ask whether a\ngiven feature is found anywhere in a region of the image.  It then\nthrows away the exact positional information.  The intuition is that\nonce a feature has been found, its exact location isn't as important\nas its rough location relative to other features.  A big benefit is\nthat there are many fewer pooled features, and so this helps reduce\nthe number of parameters needed in later layers.This convolutional architecture is quite different to the\narchitectures used in earlier chapters.  But the overall picture is\nsimilar: a network made of many simple units, whose behaviors are\ndetermined by their weights and biases.  And the overall goal is still\nthe same: to use training data to train the network's weights and\nbiases so that the network does a good job classifying input digits.\n*In fact, in this\n  experiment I actually did three separate runs training a network\n  with this architecture.  I then reported the test accuracy which\n  corresponded to the best validation accuracy from any of the three\n  runs.  Using multiple runs helps reduce variation in results, which\n  is useful when comparing many architectures, as we are doing.  I've\n  followed this procedure below, except where noted.  In practice, it\n  made little difference to the results obtained.In this architecture, we can think of the convolutional and pooling\nlayers as learning about local spatial structure in the input training\nimage, while the later, fully-connected layer learns at a more\nabstract level, integrating global information from across the entire\nimage.  This is a common pattern in convolutional neural networks.\n*This issue would have arisen in the\n  first layer if the input images were in color.  In that case we'd\n  have 3 input features for each pixel, corresponding to red, green\n  and blue channels in the input image.  So we'd allow the feature\n  detectors to have access to all color information, but only within a\n  given local receptive field.Note: This is an open-ended problem.\n    Personally, I did not find much advantage in switching to tanh,\n    although I haven't experimented exhaustively, and perhaps you may\n    find a way.  In any case, in a moment we will find an advantage in\n    switching to the rectified linear activation function, and so we\n    won't go any deeper into the use of tanh. The idea of convolutional layers is to behave in an invariant\n  way across images.  It may seem surprising, then, that our network\n  can learn more when all we've done is translate the input data.  Can\n  you explain why this is actually quite reasonable?\nIt's worth looking through these in detail. The first two digits, a 6\nand a 5, are genuine errors by our ensemble.  However, they're also\nunderstandable errors, the kind a human could plausibly make. That 6\nreally does look a lot like a 0, and the 5 looks a lot like a 3.  The\nthird image, supposedly an 8, actually looks to me more like a 9.  So\nI'm siding with the network ensemble here: I think it's done a better\njob than whoever originally drew the digit.  On the other hand, the\nfourth image, the 6, really does seem to be classified badly by our\nnetworks.And so on.  In most cases our networks' choices seem at least\nplausible, and in some cases they've done a better job classifying\nthan the original person did writing the digit.  Overall, our networks\noffer exceptional performance, especially when you consider that they\ncorrectly classified 9,967 images which aren't shown.  In that\ncontext, the few clear errors here seem quite understandable.  Even a\ncareful human makes the occasional mistake.  And so I expect that only\nan extremely careful and methodical human would do much better.  Our\nnetwork is getting near to human performance.\n*Using Theano on a GPU can be a little tricky.  In\n  particular, it's easy to make the mistake of pulling data off the\n  GPU, which can slow things down a lot.  I've tried to avoid this.\n  With that said, this code can certainly be sped up quite a bit\n  further with careful optimization of Theano's configuration.  See\n  the Theano documentation for more details.\nIn 1998, the year MNIST was introduced, it took weeks to train a\nstate-of-the-art workstation to achieve accuracies substantially worse\nthan those we can achieve using a GPU and less than an hour of\ntraining. Thus, MNIST is no longer a problem that pushes the limits of\navailable technique; rather, the speed of training means that it is a\nproblem good for teaching and learning purposes.  Meanwhile, the focus\nof research has moved on, and modern work involves much more\nchallenging image recognition problems.  In this section, I briefly\ndescribe some recent work on image recognition using neural networks.The section is different to most of the book.  Through the book I've\nfocused on ideas likely to be of lasting interest - ideas such as\nbackpropagation, regularization, and convolutional networks.  I've\ntried to avoid results which are fashionable as I write, but whose\nlong-term value is unknown. In science, such results are more often\nthan not ephemera which fade and have little lasting impact.  Given\nthis, a skeptic might say: \"well, surely the recent progress in image\nrecognition is an example of such ephemera?  In another two or three\nyears, things will have moved on.  So surely these results are only of\ninterest to a few specialists who want to compete at the absolute\nfrontier?  Why bother discussing it?\"Such a skeptic is right that some of the finer details of recent\npapers will gradually diminish in perceived importance.  With that\nsaid, the past few years have seen extraordinary improvements using\ndeep nets to attack extremely difficult image recognition tasks.\nImagine a historian of science writing about computer vision in the\nyear 2100.  They will identify the years 2011 to 2015 (and probably a\nfew years beyond) as a time of huge breakthroughs, driven by deep\nconvolutional nets.  That doesn't mean deep convolutional nets will\nstill be used in 2100, much less detailed ideas such as dropout,\nrectified linear units, and so on.  But it does mean that an important\ntransition is taking place, right now, in the history of ideas.  It's\na bit like watching the discovery of the atom, or the invention of\nantibiotics: invention and discovery on a historic scale.  And so\nwhile we won't dig down deep into details, it's worth getting some\nidea of the exciting discoveries currently being made.It's worth briefly describing KSH's network, since it has inspired\nmuch subsequent work.  It's also, as we shall see, closely related to\nthe networks we trained earlier in this chapter, albeit more\nelaborate.  KSH used a deep convolutional neural network, trained on\ntwo GPUs.  They used two GPUs because the particular type of GPU they\nwere using (an NVIDIA GeForce GTX 580) didn't have enough on-chip\nmemory to store their entire network.  So they split the network into\ntwo parts, partitioned across the two GPUs. ...the task of labeling images with 5 out of 1000\n  categories quickly turned out to be extremely challenging, even for\n  some friends in the lab who have been working on ILSVRC and its\n  classes for a while. First we thought we would put it up on [Amazon\n  Mechanical Turk]. Then we thought we could recruit paid\n  undergrads. Then I organized a labeling party of intense labeling\n  effort only among the (expert labelers) in our lab. Then I developed\n  a modified interface that used GoogLeNet predictions to prune the\n  number of categories from 1000 to only about 100. It was still too\n  hard - people kept missing categories and getting up to ranges of\n  13-15% error rates. In the end I realized that to get anywhere\n  competitively close to GoogLeNet, it was most efficient if I sat\n  down and went through the painfully long training process and the\n  subsequent careful annotation process myself... The labeling\n  happened at a rate of about 1 per minute, but this decreased over\n  time... Some images are easily recognized, while some images (such\n  as those of fine-grained breeds of dogs, birds, or monkeys) can\n  require multiple minutes of concentrated effort. I became very good\n  at identifying breeds of dogs... Based on the sample of images I\n  worked on, the GoogLeNet classification error turned out to be\n  6.8%... My own error in the end turned out to be 5.1%,\n  approximately 1.7% better.  This is a disturbing result.  The paper used a network based on the\nsame code as KSH's network - that is, just the type of network that\nis being increasingly widely used.  While such neural networks compute\nfunctions which are, in principle, continuous, results like this\nsuggest that in practice they're likely to compute functions which are\nvery nearly discontinuous.  Worse, they'll be discontinuous in ways\nthat violate our intuition about what is reasonable behavior.  That's\nconcerning.  Furthermore, it's not yet well understood what's causing\nthe discontinuity: is it something about the loss function?  The\nactivation functions used?  The architecture of the network?\nSomething else?  We don't yet know.\n  The existence of the adversarial negatives appears to be in\n  contradiction with the network’s ability to achieve high\n  generalization performance. Indeed, if the network can generalize\n  well, how can it be confused by these adversarial negatives, which\n  are indistinguishable from the regular examples? The explanation is\n  that the set of adversarial negatives is of extremely low\n  probability, and thus is never (or rarely) observed in the test set,\n  yet it is dense (much like the rational numbers), and so it is found\n  near virtually every test case.\nDespite results like this, the overall picture is encouraging.  We're\nseeing rapid progress on extremely difficult benchmarks, like\nImageNet.  We're also seeing rapid progress in the solution of\nreal-world problems, like recognizing street numbers in StreetView.\nBut while this is encouraging it's not enough just to see improvements\non benchmarks, or even real-world applications.  There are fundamental\nphenomena which we still understand poorly, such as the existence of\nadversarial images.  When such fundamental problems are still being\ndiscovered (never mind solved), it is premature to say that we're near\nsolving the problem of image recognition.  At the same time such\nproblems are an exciting stimulus to further work.I've said a little about what RNNs can do, but not so much about how\nthey work.  It perhaps won't surprise you to learn that many of the\nideas used in feedforward networks can also be used in RNNs.  In\nparticular, we can train RNNs using straightforward modifications to\ngradient descent and backpropagation.  Many other ideas used in\nfeedforward nets, ranging from regularization techniques to\nconvolutions to the activation and cost functions used, are also\nuseful in recurrent nets.  And so many of the techniques we've\ndeveloped in the book can be adapted for use with RNNs.A second reason DBNs are interesting is that they can do unsupervised\nand semi-supervised learning.  For instance, when trained with image\ndata, DBNs can learn useful features for understanding other images,\neven if the training images are unlabelled.  And the ability to do\nunsupervised learning is extremely interesting both for fundamental\nscientific reasons, and - if it can be made to work well enough -\nfor practical applications.Most of these products will fail. Inspired user interface design is\nhard, and I expect many companies will take powerful machine learning\ntechnology and use it to build insipid user interfaces.  The best\nmachine learning in the world won't help if your user interface\nconcept stinks.  But there will be a residue of products which\nsucceed.  Over time that will cause a profound change in how we relate\nto computers.  Not so long ago - let's say, 2005 - users took it\nfor granted that they needed precision in most interactions with\ncomputers.  Indeed, computer literacy to a great extent meant\ninternalizing the idea that computers are extremely literal; a single\nmisplaced semi-colon may completely change the nature of an\ninteraction with a computer.  But over the next few decades I expect\nwe'll develop many successful intention-driven user interfaces, and\nthat will dramatically change what we expect when interacting with\ncomputers.To answer the question, it helps to look at history.  Back in the\n1980s there was a great deal of excitement and optimism about neural\nnetworks, especially after backpropagation became widely known.  That\nexcitement faded, and in the 1990s the machine learning baton passed\nto other techniques, such as support vector machines.  Today, neural\nnetworks are again riding high, setting all sorts of records,\ndefeating all comers on many problems.  But who is to say that\ntomorrow some new approach won't be developed that sweeps neural\nnetworks away again?  Or perhaps progress with neural networks will\nstagnate, and nothing will immediately arise to take their place?I will make one prediction: I believe deep learning is here to stay.\nThe ability to learn hierarchies of concepts, building up multiple\nlayers of abstraction, seems to be fundamental to making sense of the\nworld.  This doesn't mean tomorrow's deep learners won't be radically\ndifferent than today's.  We could see major changes in the constituent\nunits used, in the architectures, or in the learning algorithms.\nThose changes may be dramatic enough that we no longer think of the\nresulting systems as neural networks.  But they'd still be doing deep\nlearning.Upon first hearing Conway's law, many people respond either \"Well,\nisn't that banal and obvious?\" or \"Isn't that wrong?\"  Let me start\nwith the objection that it's wrong.  As an instance of this objection,\nconsider the question: where does Boeing's accounting department show\nup in the design of the 747?  What about their janitorial department?\nTheir internal catering?  And the answer is that these parts of the\norganization probably don't show up explicitly anywhere in the 747.\nSo we should understand Conway's law as referring only to those parts\nof an organization concerned explicitly with design and engineering.What about the other objection, that Conway's law is banal and\nobvious?  This may perhaps be true, but I don't think so, for\norganizations too often act with disregard for Conway's law.  Teams\nbuilding new products are often bloated with legacy hires or,\ncontrariwise, lack a person with some crucial expertise.  Think of all\nthe products which have useless complicating features.  Or think of\nall the products which have obvious major deficiencies - e.g., a\nterrible user interface.  Problems in both classes are often caused by\na mismatch between the team that was needed to produce a good product,\nand the team that was actually assembled.  Conway's law may be\nobvious, but that doesn't mean people don't routinely ignore it.Conway's law applies to the design and engineering of systems where we\nstart out with a pretty good understanding of the likely constituent\nparts, and how to build them.  It can't be applied directly to the\ndevelopment of artificial intelligence, because AI isn't (yet) such a\nproblem: we don't know what the constituent parts are.  Indeed, we're\nnot even sure what basic questions to be asking.  In others words, at\nthis point AI is more a problem of science than of engineering.\nImagine beginning the design of the 747 without knowing about jet\nengines or the principles of aerodynamics.  You wouldn't know what\nkinds of experts to hire into your organization.  As Wernher von Braun\nput it, \"basic research is what I'm doing when I don't know what I'm\ndoing\".  Is there a version of Conway's law that applies to problems\nwhich are more science than engineering?\n*My apologies for overloading \"deep\".  I won't define\n  \"deep ideas\" precisely, but loosely I mean the kind of idea which\n  is the basis for a rich field of enquiry.  The backpropagation\n  algorithm and the germ theory of disease are both good examples.So, there are two questions to ask.  First, how powerful a set of\nideas are associated to deep learning, according to this metric of\nsocial complexity?  Second, how powerful a theory will we need, in\norder to be able to build a general artificial intelligence?As to the first question: when we look at deep learning today, it's an\nexciting and fast-paced but also relatively monolithic field.  There\nare a few deep ideas, and a few main conferences, with substantial\noverlap between several of the conferences.  And there is paper after\npaper leveraging the same basic set of ideas: using stochastic\ngradient descent (or a close variation) to optimize a cost function.\nIt's fantastic those ideas are so successful.  But what we don't yet\nsee is lots of well-developed subfields, each exploring their own sets\nof deep ideas, pushing deep learning in many directions.  And so,\naccording to the metric of social complexity, deep learning is, if\nyou'll forgive the play on words, still a rather shallow field.  It's\nstill possible for one person to master most of the deepest ideas in\nthe field.I've gone to a lot of trouble to construct an argument which is\ntentative, perhaps seems rather obvious, and which has an indefinite\nconclusion.  This will no doubt frustrate people who crave certainty.\nReading around online, I see many people who loudly assert very\ndefinite, very strongly held opinions about AI, often on the basis of\nflimsy reasoning and non-existent evidence.  My frank opinion is this:\nit's too early to say.  As the old joke goes, if you ask a scientist\nhow far away some discovery is and they say \"10 years\" (or more),\nwhat they mean is \"I've got no idea\".  AI, like controlled fusion\nand a few other technologies, has been 10 years away for 60 plus\nyears.  On the flipside, what we definitely do have in deep learning\nis a powerful technique whose limits have not yet been found, and many\nwide-open fundamental problems.  That's an exciting creative\nopportunity.","","It quickly became obvious that such an effort would require nothing less than Google-scale data and computing power. “I could try to give you some access to it,” Page told Kurzweil. “But it’s going to be very difficult to do that for an independent company.” So Page suggested that Kurzweil, who had never held a job anywhere but his own companies, join Google instead. It didn’t take Kurzweil long to make up his mind: in January he started working for Google as a director of engineering. “This is the culmination of literally 50 years of my focus on artificial intelligence,” he says.Kurzweil was attracted not just by Google’s computing resources but also by the startling progress the company has made in a branch of AI called deep learning. Deep-learning software attempts to mimic the activity in layers of neurons in the neocortex, the wrinkly 80 percent of the brain where thinking occurs. The software learns, in a very real sense, to recognize patterns in digital representations of sounds, images, and other data.The basic idea—that software can simulate the neocortex’s large array of neurons in an artificial “neural network”—is decades old, and it has led to as many disappointments as breakthroughs. But because of improvements in mathematical formulas and increasingly powerful computers, computer scientists can now model many more layers of virtual neurons than ever before.With this greater depth, they are producing remarkable advances in speech and image recognition. Last June, a Google deep-learning system that had been shown 10 million images from YouTube videos proved almost twice as good as any previous image recognition effort at identifying objects such as cats. Google also used the technology to cut the error rate on speech recognition in its latest Android mobile software. In October, Microsoft chief research officer Rick Rashid wowed attendees at a lecture in China with a demonstration of speech software that transcribed his spoken words into English text with an error rate of 7 percent, translated them into Chinese-language text, and then simulated his own voice uttering them in Mandarin. That same month, a team of three graduate students and two professors won a contest held by Merck to identify molecules that could lead to new drugs. The group used deep learning to zero in on the molecules most likely to bind to their targets.Google in particular has become a magnet for deep learning and related AI talent. In March the company bought a startup cofounded by Geoffrey Hinton, a University of Toronto computer science professor who was part of the team that won the Merck contest. Hinton, who will split his time between the university and Google, says he plans to “take ideas out of this field and apply them to real problems” such as image recognition, search, and natural-language understanding, he says.Extending deep learning into applications beyond speech and image recognition will require more conceptual and software breakthroughs, not to mention many more advances in processing power. And we probably won’t see machines we all agree can think for themselves for years, perhaps decades—if ever. But for now, says Peter Lee, head of Microsoft Research USA, “deep learning has reignited some of the grand challenges in artificial intelligence.”There have been many competing approaches to those challenges. One has been to feed computers with information and rules about the world, which required programmers to laboriously write software that is familiar with the attributes of, say, an edge or a sound. That took lots of time and still left the systems unable to deal with ambiguous data; they were limited to narrow, controlled applications such as phone menu systems that ask you to make queries by saying specific words.Neural networks, developed in the 1950s not long after the dawn of AI research, looked promising because they attempted to simulate the way the brain worked, though in greatly simplified form. A program maps out a set of virtual neurons and then assigns random numerical values, or “weights,” to connections between them. These weights determine how each simulated neuron responds—with a mathematical output between 0 and 1—to a digitized feature such as an edge or a shade of blue in an image, or a particular energy level at one frequency in a phoneme, the individual unit of sound in spoken syllables.Programmers would train a neural network to detect an object or phoneme by blitzing the network with digitized versions of images containing those objects or sound waves containing those phonemes. If the network didn’t accurately recognize a particular pattern, an algorithm would adjust the weights. The eventual goal of this training was to get the network to consistently recognize the patterns in speech or sets of images that we humans know as, say, the phoneme “d” or the image of a dog. This is much the same way a child learns what a dog is by noticing the details of head shape, behavior, and the like in furry, barking animals that other people call dogs.In the mid-1980s, Hinton and others helped spark a revival of interest in neural networks with so-called “deep” models that made better use of many layers of software neurons. But the technique still required heavy human involvement: programmers had to label data before feeding it to the network. And complex speech or image recognition required more computer power than was then available.Finally, however, in the last decade ­Hinton and other researchers made some fundamental conceptual breakthroughs. In 2006, Hinton developed a more efficient way to teach individual layers of neurons. The first layer learns primitive features, like an edge in an image or the tiniest unit of speech sound. It does this by finding combinations of digitized pixels or sound waves that occur more often than they should by chance. Once that layer accurately recognizes those features, they’re fed to the next layer, which trains itself to recognize more complex features, like a corner or a combination of speech sounds. The process is repeated in successive layers until the system can reliably recognize phonemes or objects.Like cats. Last June, Google demonstrated one of the largest neural networks yet, with more than a billion connections. A team led by Stanford computer science professor Andrew Ng and Google Fellow Jeff Dean showed the system images from 10 million randomly selected YouTube videos. One simulated neuron in the software model fixated on images of cats. Others focused on human faces, yellow flowers, and other objects. And thanks to the power of deep learning, the system identified these discrete objects even though no humans had ever defined or labeled them.What stunned some AI experts, though, was the magnitude of improvement in image recognition. The system correctly categorized objects and themes in the ­YouTube images 16 percent of the time. That might not sound impressive, but it was 70 percent better than previous methods. And, Dean notes, there were 22,000 categories to choose from; correctly slotting objects into some of them required, for example, distinguishing between two similar varieties of skate fish. That would have been challenging even for most humans. When the system was asked to sort the images into 1,000 more general categories, the accuracy rate jumped above 50 percent.Training the many layers of virtual neurons in the experiment took 16,000 computer processors—the kind of computing infrastructure that Google has developed for its search engine and other services. At least 80 percent of the recent advances in AI can be attributed to the availability of more computer power, reckons Dileep George, cofounder of the machine-learning startup Vicarious.There’s more to it than the sheer size of Google’s data centers, though. Deep learning has also benefited from the company’s method of splitting computing tasks among many machines so they can be done much more quickly. That’s a technology Dean helped develop earlier in his 14-year career at Google. It vastly speeds up the training of deep-learning neural networks as well, enabling Google to run larger networks and feed a lot more data to them.Already, deep learning has improved voice search on smartphones. Until last year, Google’s Android software used a method that misunderstood many words. But in preparation for a new release of Android last July, Dean and his team helped replace part of the speech system with one based on deep learning. Because the multiple layers of neurons allow for more precise training on the many variants of a sound, the system can recognize scraps of sound more reliably, especially in noisy environments such as subway platforms. Since it’s likelier to understand what was actually uttered, the result it returns is likelier to be accurate as well. Almost overnight, the number of errors fell by up to 25 percent—results so good that many reviewers now deem Android’s voice search smarter than Apple’s more famous Siri voice assistant.For all the advances, not everyone thinks deep learning can move artificial intelligence toward something rivaling human intelligence. Some critics say deep learning and AI in general ignore too much of the brain’s biology in favor of brute-force computing.But if it doesn’t make up for everything, the computing resources a company like Google throws at these problems can’t be dismissed. They’re crucial, say deep-learning advocates, because the brain itself is still so much more complex than any of today’s neural networks. “You need lots of computational resources to make the ideas work at all,” says Hinton.Although Google is less than forthcoming about future applications, the prospects are intriguing. Clearly, better image search would help YouTube, for instance. And Dean says deep-learning models can use phoneme data from English to more quickly train systems to recognize the spoken sounds in other languages. It’s also likely that more sophisticated image recognition could make Google’s self-driving cars much better. Then there’s search and the ads that underwrite it. Both could see vast improvements from any technology that’s better and faster at recognizing what people are really looking for—maybe even before they realize it.Kurzweil isn’t focused solely on deep learning, though he says his approach to speech recognition is based on similar theories about how the brain works. He wants to model the actual meaning of words, phrases, and sentences, including ambiguities that usually trip up computers. “I have an idea in mind of a graphical way to represent the semantic meaning of language,” he says.That in turn will require a more comprehensive way to graph the syntax of sentences. Google is already using this kind of analysis to improve grammar in translations. Natural-language understanding will also require computers to grasp what we humans think of as common-sense meaning. For that, Kurzweil will tap into the Knowledge Graph, Google’s catalogue of some 700 million topics, locations, people, and more, plus billions of relationships among them. It was introduced last year as a way to provide searchers with answers to their queries, not just links.Finally, Kurzweil plans to apply deep-learning algorithms to help computers deal with the “soft boundaries and ambiguities in language.” If all that sounds daunting, it is. “Natural-language understanding is not a goal that is finished at some point, any more than search,” he says. “That’s not a project I think I’ll ever finish.”Though Kurzweil’s vision is still years from reality, deep learning is likely to spur other applications beyond speech and image recognition in the nearer term. For one, there’s drug discovery. The surprise victory by Hinton’s group in the Merck contest clearly showed the utility of deep learning in a field where few had expected it to make an impact.That’s not all. Microsoft’s Peter Lee says there’s promising early research on potential uses of deep learning in machine vision—technologies that use imaging for applications such as industrial inspection and robot guidance. He also envisions personal sensors that deep neural networks could use to predict medical problems. And sensors throughout a city might feed deep-learning systems that could, for instance, predict where traffic jams might occur.In a field that attempts something as profound as modeling the human brain, it’s inevitable that one technique won’t solve all the challenges. But for now, this one is leading the way in artificial intelligence. “Deep learning,” says Dean, “is a really powerful metaphor for learning about the world.”","We’ll show you how to train and optimize basic neural networks, convolutional neural networks, and long short term memory networks. Complete learning systems in TensorFlow will be introduced via projects and assignments. You will learn to solve new classes of problems that were once thought prohibitively challenging, and come to better appreciate the complex nature of human intelligence as you solve these same problems effortlessly using deep learning methods. This is an intermediate to advanced level course. Prior to taking this course, and in addition to the prerequisites and requirements outlined for the Machine Learning Engineer Nanodegree program, you should possess the following experience and skills:Deep learning methods are becoming exponentially more important due to their demonstrated success at tackling complex learning problems. At the same time, increasing access to high-performance computing resources and state-of-the-art open-source libraries are making it more and more feasible for enterprises, small firms, and individuals to use these methods. Mastering deep learning accordingly positions you at the very forefront of one of the most promising, innovative, and influential emergent technologies, and opens up tremendous new career opportunities. For Data Analysts, Data Scientists, Machine Learning Engineers, and students in a Machine Learning/Artificial Intelligence curriculum, this represents a rarefied opportunity to enhance your Machine Learning portfolio with an advanced, yet broadly applicable, collection of vital techniques..icon-chat-bubble-call-to-action-cls-1{fill:#182430;}.icon-chat-bubble-call-to-action-cls-2{font-size:14px;fill:#fff;font-family: \"Open Sans\", sans-serif;}.icon-chat-bubble-call-to-action-cls-3,.icon-chat-bubble-call-to-action-cls-4{fill:none;stroke:#fff;stroke-miterlimit:10;stroke-width:2px;}.icon-chat-bubble-call-to-action-cls-3{stroke-linecap:round;}","\nTraditional machine learning uses handwritten feature extraction and modality-specific machine learning algorithms to label images or recognize voices. However, this method has several drawbacks in both time-to-solution and accuracy.","Thank you for visiting nature.com. You are using a browser version with\n    limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off\n    compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site\n    without styles and JavaScript.Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users' interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning.Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input.Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.The most common form of machine learning, deep or not, is supervised learning. Imagine that we want to build a system that can classify images as containing, say, a house, a car, a person or a pet. We first collect a large data set of images of houses, cars, people and pets, each labelled with its category. During training, the machine is shown an image and produces an output in the form of a vector of scores, one for each category. We want the desired category to have the highest score of all categories, but this is unlikely to happen before training. We compute an objective function that measures the error (or distance) between the output scores and the desired pattern of scores. The machine then modifies its internal adjustable parameters to reduce this error. These adjustable parameters, often called weights, are real numbers that can be seen as 'knobs' that define the input–output function of the machine. In a typical deep-learning system, there may be hundreds of millions of these adjustable weights, and hundreds of millions of labelled examples with which to train the machine.To properly adjust the weight vector, the learning algorithm computes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. The weight vector is then adjusted in the opposite direction to the gradient vector.The objective function, averaged over all the training examples, can be seen as a kind of hilly landscape in the high-dimensional space of weight values. The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.Many of the current practical applications of machine learning use linear classifiers on top of hand-engineered features. A two-class linear classifier computes a weighted sum of the feature vector components. If the weighted sum is above a threshold, the input is classified as belonging to a particular category.A deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning, and many of which compute non-linear input–output mappings. Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation. With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate functions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations such as the background, pose, lighting and surrounding objects.In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities. It was widely thought that learning useful, multistage, feature extractors with little prior knowledge was infeasible. In particular, it was commonly thought that simple gradient descent would get trapped in poor local minima — weight configurations for which no small change would reduce the average error.ConvNets are designed to process data that come in the form of multiple arrays, for example a colour image composed of three 2D arrays containing pixel intensities in the three colour channels. Many data modalities are in the form of multiple arrays: 1D for signals and sequences, including language; 2D for images or audio spectrograms; and 3D for video or volumetric images. There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right). Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions. Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output. ReLU, rectified linear unit.Although the role of the convolutional layer is to detect local conjunctions of features from the previous layer, the role of the pooling layer is to merge semantically similar features into one. Because the relative positions of the features forming a motif can vary somewhat, reliably detecting the motif can be done by coarse-graining the position of each feature. A typical pooling unit computes the maximum of a local patch of units in one feature map (or in a few feature maps). Neighbouring pooling units take input from patches that are shifted by more than one row or column, thereby reducing the dimension of the representation and creating an invariance to small shifts and distortions. Two or three stages of convolution, non-linearity and pooling are stacked, followed by more convolutional and fully-connected layers. Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.Deep neural networks exploit the property that many natural signals are compositional hierarchies, in which higher-level features are obtained by composing lower-level ones. In images, local combinations of edges form motifs, motifs assemble into parts, and parts form objects. Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences. The pooling allows representations to vary very little when elements in the previous layer vary in position and appearance.Recent ConvNet architectures have 10 to 20 layers of ReLUs, hundreds of millions of weights, and billions of connections between units. Whereas training such large networks could have taken weeks only two years ago, progress in hardware, software and algorithm parallelization have reduced training times to a few hours.The performance of ConvNet-based vision systems has caused most major technology companies, including Google, Facebook, Microsoft, IBM, Yahoo!, Twitter and Adobe, as well as a quickly growing number of start-ups to initiate research and development projects and to deploy ConvNet-based image understanding products and services.The issue of representation lies at the heart of the debate between the logic-inspired and the neural-network-inspired paradigms for cognition. In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbol instances. It has no internal structure that is relevant to its use; and to reason with symbols, they must be bound to the variables in judiciously chosen rules of inference. By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast 'intuitive' inference that underpins effortless commonsense reasoning.This joint paper from the major speech recognition laboratories, summarizing the breakthrough achieved with deep learning on the task of phonetic classification for automatic speech recognition, was the first major industrial application of deep learning.This paper showed state-of-the-art machine translation results with the architecture introduced in ref. 72, with a recurrent network trained to read a sentence in one language, produce a semantic representation of its meaning, and generate a translation in another language.This report demonstrated that the unsupervised pre-training method introduced in ref. 32 significantly improves performance on test data and generalizes the method to other unsupervised representation-learning techniques, such as auto-encoders.This overview paper on the principles of end-to-end training of modular systems such as deep neural networks using gradient-based optimization showed how neural networks (and in particular convolutional nets) can be combined with search or inference mechanisms to model complex outputs that are interdependent, such as sequences of characters associated with the content of a document.The authors would like to thank the Natural Sciences and Engineering Research Council of Canada, the Canadian Institute For Advanced Research (CIFAR), the National Science Foundation and Office of Naval Research for support. Y.L. and Y.B. are CIFAR fellows.This joint paper from the major speech recognition laboratories, summarizing the breakthrough achieved with deep learning on the task of phonetic classification for automatic speech recognition, was the first major industrial application of deep learning.This paper showed state-of-the-art machine translation results with the architecture introduced in ref. 72, with a recurrent network trained to read a sentence in one language, produce a semantic representation of its meaning, and generate a translation in another language.This report demonstrated that the unsupervised pre-training method introduced in ref. 32 significantly improves performance on test data and generalizes the method to other unsupervised representation-learning techniques, such as auto-encoders.This overview paper on the principles of end-to-end training of modular systems such as deep neural networks using gradient-based optimization showed how neural networks (and in particular convolutional nets) can be combined with search or inference mechanisms to model complex outputs that are interdependent, such as sequences of characters associated with the content of a document.","Toronto's win provided a roadmap for the future of deep learning. In the years since, the biggest names on the 'net—including Facebook, Google, Twitter, and Microsoft—have used similar tech to build computer vision systems that can match and even surpass humans. \"We can't claim that our system 'sees' like a person does,\" says Peter Lee, the head of research at Microsoft. \"But what we can say is that for very specific, narrowly defined tasks, we can learn to be as good as humans.\"Roughly speaking, neural nets use hardware and software to approximate the web of neurons in the human brain. This idea dates to the 1980s, but in 2012, Krizhevsky and Hinton advanced the technology by running their neural nets atop graphics processing units, or GPUs. These specialized chips were originally designed to render images for games and other highly graphical software, but as it turns out, they're also suited to the kind of math that drives neural nets. Google, Facebook, Twitter, Microsoft, and so many others now use GPU-powered-AI to handle image recognition and so many others tasks, from Internet search to security. Krizhevsky and Hinton joined the staff at Google.Now, the latest ImageNet winner is pointing to what could be another step in the evolution of computer vision—and the wider field of artificial intelligence. Last month, a team of Microsoft researchers took the ImageNet crown using a new approach they call a deep residual network. The name doesn't quite describe it. They've designed a neural net that's significantly more complex than typical designs—one that spans 152 layers of mathematical operations, compared to the typical six or seven. It shows that, in the years to come, companies like Microsoft will be able to use vast clusters of GPUs and other specialized chips to significantly improve not only image recognition but other AI services, including systems that recognize speech and even understand language as we humans naturally speak it.Deep neural networks are arranged in layers. Each layer is a different set of mathematical operations—aka algorithms. The output of one layer becomes the input of the next. Loosely speaking, if a neural network is designed for image recognition, one layer will look for a particular set of features in an image—edges or angles or shapes or textures or the like—and the next will look for another set. These layers are what make these neural networks deep. \"Generally speaking, if you make these networks deeper, it becomes easier for them to learn,\" says Alex Berg, a researcher at the University of North Carolina who helps oversee the ImageNet competition.In the past, according Lee and researchers outside of Microsoft, this sort of very deep neural net wasn't feasible. Part of the problem was that as your mathematical signal moved from layer to layer, it became diluted and tended to fade. As Lee explains, Microsoft solved this problem by building a neural net that skips certain layers when it doesn't need them, but uses them when it does. \"When you do this kind of skipping, you're able to preserve the strength of the signal much further,\" Lee says, \"and this is turning out to have a tremendous, beneficial impact on accuracy.\"As Jian Sun explains it, researchers can identify a promising arrangement for massive neural networks, and then the system can cycle through a range of similar possibilities until it settles on this best one. \"In most cases, after a number of tries, the researchers learn [something], reflect, and make a new decision on the next try,\" he says. \"You can view this as 'human-assisted search.'\"As Peter Lee and Jian Sun describe it, such an approach isn't exactly \"brute forcing\" the problem. \"With very very large amounts of compute resources, one could fantasize about a gigantic 'natural selection' setup where evolutionary forces help direct a brute-force search through a huge space of possibilities,\" Lee says. \"The world doesn't have those computing resources available for such a thing...For now, we will still depend on really smart researchers like Jian.\"But Lee does say that, thanks to new techniques and computer data centers filled with GPU machines, the realm of possibilities for deep learning are enormous. A big part of the company's task is just finding the time and the computing power needed to explore these possibilities. \"This work has dramatically exploded the design space. The amount of ground to cover, in terms of scientific investigation, has become exponentially larger,\" Lee says. And this extends well beyond image recognition, into speech recognition, natural language understanding, and other tasks.Indeed, Gibson says that deep learning has become more of \"a hardware problem.\" Yes, we still need top researchers to guide the creation of neural networks, but more and more, finding new paths is a matter of brute-forcing new algorithms across ever more powerful collections of hardware. As Gibson point out, though these deep neural nets work extremely well, we don't quite know why they work. The trick lies in finding the complex combination of algorithms that work the best. More and better hardware can shorten the path.The end result is that the companies that can build the most powerful networks of hardware are the companies that will come out ahead. That would be Google and Facebook and Microsoft. Those that are good at deep learning today will only get better.","If you are just starting out in the field of deep learning or you had some experience with neural networks some time ago, you may be confused. I know I was confused initially and so were many of my colleagues and friends who learned and used neural networks in the 1990s and early 2000s.He also commented on the important point that it is all about scale. That as we construct larger neural networks and train them with more and more data, their performance continues to increase. This is generally different to other machine learning techniques that reach a plateau in performance.Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level featuresDeep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. Automatically learning features at multiple levels of abstraction allow a system to learn complex functions mapping the input to the output directly from data, without depending completely on human-crafted features.The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning.This is an important book and will likely become the definitive resource for the field for some time. The book goes on to describe multilayer perceptrons as an algorithm used in the field of deep learning, giving the idea that deep learning has subsumed artificial neural networks.In the same article, they make an interesting comment that meshes with Andrew Ng’s comment about the recent increase in compute power and access to large datasets that has unleashed the untapped capability of neural networks when used at larger scale.It has been obvious since the 1980s that backpropagation through deep autoencoders would be very effective for nonlinear dimensionality reduction, provided that computers were fast enough, data sets were big enough, and the initial weights were close enough to a good solution. All three conditions are now satisfied.The descriptions of deep learning in the Royal Society talk are very backpropagation centric as you would expect. Interesting, he gives 4 reasons why backpropagation (read “deep learning”) did not take off last time around in the 1990s. The first two points match comments by Andrew Ng above about datasets being too small and computers being too slow.At which problem depth does Shallow Learning end, and Deep Learning begin? Discussions with DL experts have not yet yielded a conclusive response to this question. […], let me just define for the purposes of this overview: problems of depth > 10 require Very Deep Learning.To achieve this,we developed a novel agent, a deep Q-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network known as deep neural networks. Notably, recent advances in deep neural networks, in which several layers of nodes are used to build up progressively more abstract representations of the data, have made it possible for artificial neural networks to learn concepts such as object categories directly from raw sensory data.Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. […] The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure.Although early approaches published by Hinton and collaborators focus on greedy layerwise training and unsupervised methods like autoencoders, modern state-of-the-art deep learning is focused on training deep (many layered) neural network models using the backpropagation algorithm. The most popular techniques are:CNN would be extremely better than SVM if and only if you have enough data. The reason that CNN would be better is that CNN work as an automatic feature extractor and you won’t need to bother yourself of feature selection and wondering if the extracted feature would weather work with the model or not. CNN extracts all possible features, from low-level features like edges to higher-level features like faces and objects.ECG interpretation may be a good problem for CNNs in that they are images. Another project is the development of  a Consultant in Cardiovascular Disease analogous to MYCIN, an Infectious Disease Consultatant developed by Shortliffe & Buchanan @ Stanford ~ 40 years ago which was Rule Based.Thank you so much for your post. I am trying to solve an open problem with regards to embedded short text messages on the social media which are abbreviation, symbol and others. For instance, take bf can be interpret as boy friend or best friend. The input can be represent as  character but how can someone encode this as input in neural network, so it can learn and output the target at the same time. Please help.Hi… I am an average developer in a developing country and my opinion is “yes”… if you find a way to get all these “disconnected” data together than you can help on gathering these data to make it easier for developing countries not to make the same mistakes as developed countries… thus bringing the cost down on “becoming” a developed country without the cost… the “research” exist… the implementation is the problem…So my actual question: the “data” according to me is available -> “internet” BUT do we (humanity currently) already have the computational ability to make “sense” of the data via these algorithms AND are the software developed in such a way to ignore human approval?Based on my readings so far, I feel predictive analytics is at the core of both machine learning and deep learning is an approach for predictive analytics with accuracy that scales with more data and training. Would like to hear your thoughts on this.I am thinking about a project (just for my hobby) of designing a stabilization controller for a DIY Quadrotor. Do you have any advice on how and where I should start off? Can algorithms like SVM be used in this specific purpose? Is micro controller (like Arduino) able to handle this problem? if we use hierarchal training algorithm such as we use unsupervised learning autoencoder with bottleneck (hidden layer, 10,2,10) for training then use the supervised learning with same autoencoder architecture ( hidden layer, 10,2,10) to tune the unsupervised model parameter (weights, bias). These training processes are performed separately.Sir, It is a good intro to deep learning. i’m planning to do phd in diagnosis of heart disease using deep learning. I have data’s of features. I don’t know how to classify those data. Can you please refer some material for numerical data classification using tensor flow.What does “bigger models” mean? Are there more equations in the model? Are there more variables in the model? Are there more for loops? How exactly is the model “bigger”? What’s an example of a “not big model” and why is that worse? And what exactly is meant by the term “model” in this field? I’m still not clear on that. Like what exactly are the characteristics that make something a “model” and why is another general programming algorithm like, say, a loop that divides numbers successively to determine if an integer is prime, not a “model”? To me that sounds like a “model” for determining if a number is prime, so what is meant in this field by “model”? What are the inherent properties that make something a “model”? Is a model a type of algorithm? Is it a class in object-oriented design? What’s a “model” and what;s a “bigger model”?And then what’s the “bigger model” you refer to in this article? Are there more weights and more structure in the training algorithm? How is that achieved? Do you plug in more equations and more variables and decision parameters/whatever? How do you know what additional equations and parameters to plug in, and how do you know those are the right ones as opposed to others? Or does “bigger models” simply refer to running on more overall data, and the training algorithm is the same with the same amount of equations/variables/computations per training example?","Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.With classification, deep learning is able to establish correlations between, say, pixels in an image and the name of a person. You might call this a static prediction. By the same token, exposed to enough of the right data, deep learning is able to establish correlations between present events and future events. The future event is like the label in a sense. Deep learning doesn’t necessarily care about time, or the fact that something hasn’t happened yet. Given a time series, deep learning may read a string of number and predict the number most likely to occur next.Traditional machine learning relies on shallow nets, composed of one input and one output layer, and at most one hidden layer in between. More than three layers (including input and output) qualifies as “deep” learning. So deep is a strictly defined, technical term that means more than one hidden layer.In deep-learning networks, each layer of nodes trains on a distinct set of features based on the previous layer’s output. The further you advance into the neural net, the more complex the features your nodes can recognize, since they aggregate and recombine features from the previous layer.For example, deep learning can take a million images, and cluster them according to their similarities: cats in one corner, ice breakers in another, and in a third all the photos of your grandmother. This is the basis of so-called smart photo albums.Now apply that same idea to other data types: Deep learning might cluster raw text such as emails or news articles. Emails full of angry complaints might cluster in one corner of the vector space, while satisfied customers, or spambot messages, might cluster in others. This is the basis of various messaging filters, and can be used in customer-relationship management (CRM). The same applies to voice messages.\nWith time series, data might cluster around normal/healthy behavior and anomalous/dangerous behavior. If the time series data is being generated by a smart phone, it will provide insight into users’ health and habits; if it is being generated by an autopart, it might be used to prevent catastrophic breakdowns.When training on unlabeled data, each node layer in a deep network learns features automatically by repeatedly trying to reconstruct the input from which it draws its samples, attempting to minimize the difference between the network’s guesses and the probability distribution of the input data itself. Restricted Boltzmann machines, for examples, create so-called reconstructions in this manner.In the process, these networks learn to recognize correlations between certain relevant features and optimal results – they draw connections between feature signals and what those features represent, whether it be a full reconstruction, or with labeled data.A deep-learning network trained on labeled data can then be applied to unstructured data, giving it access to much more input than machine-learning nets. This is a recipe for higher performance: the more data a net can train on, the more accurate it is likely to be. (Bad algorithms trained on lots of data can outperform good algorithms trained on very little.) Deep learning’s ability to process and learn from huge quantities of unlabeled data give it a distinct advantage over previous algorithms.Deep-learning networks end in an output layer: a logistic, or softmax, classifier that assigns a likelihood to a particular outcome or label. We call that predictive, but it is predictive in a broad sense. Given raw data in the form of an image, a deep-learning network may decide, for example, that the input data is 90 percent likely to represent a person.Our goal in using a neural net is to arrive at the point of least error as fast as possible. We are running a race, and the race is around a track, so we pass the same points repeatedly in a loop. The starting line for the race is the state in which our weights are initialized, and the finish line is the state of those parameters when they are capable of producing accurate classifications and predictions.The race itself involves many steps, and each of those steps resembles the steps before and after. Just like a runner, we will engage in a repetitive act over and over to arrive at the finish. Each step for a neural network involves a guess, an error measurement and a slight update in its weights, an incremental adjustment to the coefficients.A collection of weights, whether they are in their start or end state, is also called a model, because it is an attempt to model data’s relationship to ground-truth labels, to grasp the data’s structure. Models normally start out bad and end up less bad, changing over time as the neural network updates its parameters.This is because a neural network is born in ignorance. It does not know which weights and biases will translate the input best to make the correct guesses. It has to start out with a guess, and then try to make better guesses sequentially as it learns from its mistakes. (You can think of a neural network as a miniature enactment of the scientific method, testing hypotheses and trying again – only it is the scientific method with a blindfold on.)The three pseudo-mathematical formulas above account for the three key functions of neural networks: scoring input, calculating loss and applying an update to the model – to begin the three-step process over again. A neural network is a corrective feedback loop, rewarding weights that support its correct guesses, and punishing weights that lead it to err.Now, that form of multiple linear regression is happening at every node of a neural network. For each node of a single layer, input from each node of the previous layer is recombined with input from every other node. That is, the inputs are mixed in different proportions, according to their coefficients, which are different leading into each node of the subsequent layer. In this way, a net tests which combination of input is significant as it tries to reduce error.The nonlinear transforms at each node are usually s-shaped functions similar to logistic regression. They go by the names of sigmoid (the Greek word for “S”), tanh, hard tanh, etc., and they shaping the output of each node. The output of all nodes, each squashed into an s-shaped space between 0 and 1, is then passed as input to the next layer in a feed forward neural network, and so on until the signal reaches the final layer of the net, where decisions are made.Gradient is another word for slope, and slope, in its typical form on an x-y graph, represents how two variables relate to each other: rise over run, the change in money over the change in time, etc. In this particular case, the slope we care about describes the relationship between the network’s error and a single weight; i.e. that is, how does the error vary as the weight is adjusted.To put a finer point on it, which weight will produce the least error? Which one correctly represents the signals contained in the input data, and translates them to a correct classification? Which one can hear “nose” in an input image, and know that should be labeled as a face and not a frying pan?The activation function determines what output a node will generate base upon its input. Sigmoid activation functions had been very populare, ReLU is currently very popular. In DeepLearnging4J the activation function is set at the layer level and applies to all neurons in that layer.On a deep neural network of many layers, the final layer has a particular role. When dealing with labeled input, the output layer classifies each example, applying the most likely label. Each node on the output layer represents one label, and that node turns on or off according to the strength of the signal it receives from the previous layer’s input and parameters.While neural networks working with labeled data produce binary output, the input they receive is often continuous. That is, the signals that the network receives as input will span a range of values and include any number of metrics, depending on the problem it seeks to solve.For example, a recommendation engine has to make a binary decision about whether to serve an ad or not. But the input it bases its decision on could include how much a customer has spent on Amazon in the last week, or how often that customer visits the site.The mechanism we use to convert continuous signals into binary output is called logistic regression. The name is unfortunate, since logistic regression is used for classification rather than regression in the linear sense that most people are familiar with. It calculates the probability that a set of inputs match the label.With this layer, we can set a decision threshold above which an example is labeled 1, and below which it is not. You can set different thresholds as you prefer – a low threshold will increase the number of false positives, and a higher one will increase the number of false negatives – depending on which side you would like to err.In some circles, neural networks are thought of as “brute force” AI, because they start with a blank slate and hammer their way through to an accurate model. They are effective, but to some eyes inefficient in their approach to modeling, which can’t make assumptions about functional dependencies between output and input.That said, gradient descent is not recombining every weight with every other to find the best match – its method of pathfinding shrinks the relevant weight space, and therefore the number of updates and required computation, by many orders of magnitude.Here, then, is Darwin's dangerous idea: the algorithmic level is the level that best accounts for the speed of the antelope, the wing of the eagle, the shape of the orchid, the diversity of species, and all the other occasions for wonder in the world of nature. - Daniel C. Dennett","Each sequence produces an error as the sum of the deviations of all target signals from the corresponding activations computed by the network. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.The stopping criterion is evaluated by the fitness function as it gets the reciprocal of the mean-squared-error from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared-error."]